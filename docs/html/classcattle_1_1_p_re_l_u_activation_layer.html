<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>C-ATTL3: cattle::PReLUActivationLayer&lt; Scalar, Rank &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">C-ATTL3
   &#160;<span id="projectnumber">0.5</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacecattle.html">cattle</a></li><li class="navelem"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">PReLUActivationLayer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="classcattle_1_1_p_re_l_u_activation_layer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">cattle::PReLUActivationLayer&lt; Scalar, Rank &gt; Class Template Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p><code>#include &lt;<a class="el" href="_layer_8hpp_source.html">Layer.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for cattle::PReLUActivationLayer&lt; Scalar, Rank &gt;:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classcattle_1_1_p_re_l_u_activation_layer.png" usemap="#cattle::PReLUActivationLayer_3C_20Scalar_2C_20Rank_20_3E_map" alt=""/>
  <map id="cattle::PReLUActivationLayer_3C_20Scalar_2C_20Rank_20_3E_map" name="cattle::PReLUActivationLayer_3C_20Scalar_2C_20Rank_20_3E_map">
<area href="classcattle_1_1_activation_layer.html" alt="cattle::ActivationLayer&lt; Scalar, Rank &gt;" shape="rect" coords="0,56,273,80"/>
<area href="classcattle_1_1_layer.html" alt="cattle::Layer&lt; Scalar, Rank &gt;" shape="rect" coords="0,0,273,24"/>
</map>
 </div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a30914840f4fc20d25805b8a3ac0c6e51"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a30914840f4fc20d25805b8a3ac0c6e51">PReLUActivationLayer</a> (const <a class="el" href="classcattle_1_1_dimensions.html">Dimensions</a>&lt; std::size_t, Rank &gt; &amp;dims, <a class="el" href="namespacecattle.html#a083ab63c64a5c935b2d2cd3f03c4e27a">ParamRegSharedPtr</a>&lt; Scalar &gt; param_reg=Root::NO_PARAM_REG, Scalar init_alpha=1e-1, Scalar max_norm_constraint=0)</td></tr>
<tr class="separator:a30914840f4fc20d25805b8a3ac0c6e51"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a822da21b34e644180efda947c185e13e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classcattle_1_1_layer.html">Layer</a>&lt; Scalar, Rank &gt; *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a822da21b34e644180efda947c185e13e">clone</a> () const</td></tr>
<tr class="separator:a822da21b34e644180efda947c185e13e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a26107800b142f95b0d34a9519b801e9c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a26107800b142f95b0d34a9519b801e9c">init</a> ()</td></tr>
<tr class="separator:a26107800b142f95b0d34a9519b801e9c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcattle_1_1_activation_layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcattle_1_1_activation_layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcattle_1_1_activation_layer.html">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a></td></tr>
<tr class="memitem:a4550c77e6341cd77a62a751eb129e44e inherit pub_methods_classcattle_1_1_activation_layer"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="classcattle_1_1_dimensions.html">Dimensions</a>&lt; std::size_t, Rank &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_activation_layer.html#a4550c77e6341cd77a62a751eb129e44e">get_input_dims</a> () const</td></tr>
<tr class="separator:a4550c77e6341cd77a62a751eb129e44e inherit pub_methods_classcattle_1_1_activation_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a30f2215af291a00e008fa09de8a32cf7 inherit pub_methods_classcattle_1_1_activation_layer"><td class="memItemLeft" align="right" valign="top">const <a class="el" href="classcattle_1_1_dimensions.html">Dimensions</a>&lt; std::size_t, Rank &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_activation_layer.html#a30f2215af291a00e008fa09de8a32cf7">get_output_dims</a> () const</td></tr>
<tr class="separator:a30f2215af291a00e008fa09de8a32cf7 inherit pub_methods_classcattle_1_1_activation_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcattle_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcattle_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcattle_1_1_layer.html">cattle::Layer&lt; Scalar, Rank &gt;</a></td></tr>
<tr class="memitem:a92963ac4a339d9174a3aef3dbf3c271f inherit pub_methods_classcattle_1_1_layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_layer.html#a92963ac4a339d9174a3aef3dbf3c271f">is_parametric</a> ()</td></tr>
<tr class="separator:a92963ac4a339d9174a3aef3dbf3c271f inherit pub_methods_classcattle_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:ae2324030acae7265684f196f6c20343b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classcattle_1_1_layer.html">Layer</a>&lt; Scalar, Rank &gt; *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#ae2324030acae7265684f196f6c20343b">clone_with_shared_params</a> () const</td></tr>
<tr class="separator:ae2324030acae7265684f196f6c20343b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ac09416efe5d2cd6cc73df9890e9725"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a7ac09416efe5d2cd6cc73df9890e9725">empty_cache</a> ()</td></tr>
<tr class="separator:a7ac09416efe5d2cd6cc73df9890e9725"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7bb2f7bd02261af5aae033d9727c00c8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a7bb2f7bd02261af5aae033d9727c00c8">regularize</a> ()</td></tr>
<tr class="separator:a7bb2f7bd02261af5aae033d9727c00c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c7f81f3fd23a585d6f6ec11b136b058"><td class="memItemLeft" align="right" valign="top">Scalar&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a3c7f81f3fd23a585d6f6ec11b136b058">get_regularization_penalty</a> ()</td></tr>
<tr class="separator:a3c7f81f3fd23a585d6f6ec11b136b058"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22b686f15e500942fb894063ae65bda8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a22b686f15e500942fb894063ae65bda8">enforce_constraints</a> ()</td></tr>
<tr class="separator:a22b686f15e500942fb894063ae65bda8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c4c775a8f711ecce2636d71b4d31c77"><td class="memItemLeft" align="right" valign="top">Root::Data&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a2c4c775a8f711ecce2636d71b4d31c77">pass_forward</a> (typename Root::Data in, bool training)</td></tr>
<tr class="separator:a2c4c775a8f711ecce2636d71b4d31c77"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a50562bf1416371801c5abba6a1b206b0"><td class="memItemLeft" align="right" valign="top">Root::Data&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html#a50562bf1416371801c5abba6a1b206b0">pass_back</a> (typename Root::Data out_grads)</td></tr>
<tr class="separator:a50562bf1416371801c5abba6a1b206b0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_methods_classcattle_1_1_activation_layer"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classcattle_1_1_activation_layer')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classcattle_1_1_activation_layer.html">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a></td></tr>
<tr class="memitem:a3733446d064f63f86f8a48bd30c0dbcb inherit pro_methods_classcattle_1_1_activation_layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_activation_layer.html#a3733446d064f63f86f8a48bd30c0dbcb">is_input_layer</a> () const</td></tr>
<tr class="separator:a3733446d064f63f86f8a48bd30c0dbcb inherit pro_methods_classcattle_1_1_activation_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34972577a520dc5e4d34d40c448f127b inherit pro_methods_classcattle_1_1_activation_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_activation_layer.html#a34972577a520dc5e4d34d40c448f127b">set_input_layer</a> (bool input_layer)</td></tr>
<tr class="separator:a34972577a520dc5e4d34d40c448f127b inherit pro_methods_classcattle_1_1_activation_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab97e38ad519af40007f659120f3f7fe1 inherit pro_methods_classcattle_1_1_activation_layer"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecattle.html#a1d78623a47279d516750a44dbad6090b">Matrix</a>&lt; Scalar &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_activation_layer.html#ab97e38ad519af40007f659120f3f7fe1">get_params</a> ()</td></tr>
<tr class="separator:ab97e38ad519af40007f659120f3f7fe1 inherit pro_methods_classcattle_1_1_activation_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af9822a94073cb72e2db4de62c2126d54 inherit pro_methods_classcattle_1_1_activation_layer"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacecattle.html#a1d78623a47279d516750a44dbad6090b">Matrix</a>&lt; Scalar &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcattle_1_1_activation_layer.html#af9822a94073cb72e2db4de62c2126d54">get_params_grad</a> ()</td></tr>
<tr class="separator:af9822a94073cb72e2db4de62c2126d54 inherit pro_methods_classcattle_1_1_activation_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;typename Scalar, std::size_t Rank&gt;<br />
class cattle::PReLUActivationLayer&lt; Scalar, Rank &gt;</h3>

<p>A class template representing a parametric rectified linear unit (PReLU) activation function. PReLU layers are Leaky ReLU activation functions with element-wise, learnable alphas. PReLU activation functions are not theoretically differentiable. </p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a30914840f4fc20d25805b8a3ac0c6e51"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a30914840f4fc20d25805b8a3ac0c6e51">&#9670;&nbsp;</a></span>PReLUActivationLayer()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::<a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">PReLUActivationLayer</a> </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classcattle_1_1_dimensions.html">Dimensions</a>&lt; std::size_t, Rank &gt; &amp;&#160;</td>
          <td class="paramname"><em>dims</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecattle.html#a083ab63c64a5c935b2d2cd3f03c4e27a">ParamRegSharedPtr</a>&lt; Scalar &gt;&#160;</td>
          <td class="paramname"><em>param_reg</em> = <code>Root::NO_PARAM_REG</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Scalar&#160;</td>
          <td class="paramname"><em>init_alpha</em> = <code>1e-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Scalar&#160;</td>
          <td class="paramname"><em>max_norm_constraint</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">dims</td><td>The dimensionality of the input tensor. </td></tr>
    <tr><td class="paramname">param_reg</td><td>The regularization function to apply to the layer's parameters. </td></tr>
    <tr><td class="paramname">init_alpha</td><td>The initial factor by which negative inputs are to be scaled. </td></tr>
    <tr><td class="paramname">max_norm_constraint</td><td>An optional max-norm constraint. If it is 0 or less, no constraint is applied. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a822da21b34e644180efda947c185e13e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a822da21b34e644180efda947c185e13e">&#9670;&nbsp;</a></span>clone()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classcattle_1_1_layer.html">Layer</a>&lt;Scalar,Rank&gt;* <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::clone </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>A constant method implementing the clone pattern.</p>
<dl class="section return"><dt>Returns</dt><dd>A pointer to a copy of the instance. The instance does not take ownership of the returned pointer (i.e. the caller is responsible for deleting it). </dd></dl>

<p>Implements <a class="el" href="classcattle_1_1_layer.html#a46db46c62f3d46c6bbef5a482d7fcb00">cattle::Layer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="ae2324030acae7265684f196f6c20343b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2324030acae7265684f196f6c20343b">&#9670;&nbsp;</a></span>clone_with_shared_params()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classcattle_1_1_layer.html">Layer</a>&lt;Scalar,Rank&gt;* <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::clone_with_shared_params </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It returns a clone of the layer instance using a reference to the original's parameters.</p>
<dl class="section return"><dt>Returns</dt><dd>A clone of the original layer instance sharing the same parameters with the original. </dd></dl>

<p>Implements <a class="el" href="classcattle_1_1_layer.html#a725b8784f71f40c162f25a8820c97473">cattle::Layer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a7ac09416efe5d2cd6cc73df9890e9725"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7ac09416efe5d2cd6cc73df9890e9725">&#9670;&nbsp;</a></span>empty_cache()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::empty_cache </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It empties the layer's caches such as those required for the derivation of the function represented by the layer. </p>

<p>Reimplemented from <a class="el" href="classcattle_1_1_activation_layer.html#a3ea3acc95aef44eac905481020f3f946">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a22b686f15e500942fb894063ae65bda8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22b686f15e500942fb894063ae65bda8">&#9670;&nbsp;</a></span>enforce_constraints()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::enforce_constraints </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It applies constraints such as max-norm to the parameters of the layer (if applicable). </p>

<p>Reimplemented from <a class="el" href="classcattle_1_1_activation_layer.html#a2722517029fa18cb0ee1b9038a643da9">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a3c7f81f3fd23a585d6f6ec11b136b058"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c7f81f3fd23a585d6f6ec11b136b058">&#9670;&nbsp;</a></span>get_regularization_penalty()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Scalar <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::get_regularization_penalty </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It calculates the regularization penalty of the layer's parameters. If the layer is not parametric, 0 is returned.</p>
<dl class="section return"><dt>Returns</dt><dd>A scalar representing the penalty on the magnitude of the layer's parameters. </dd></dl>

<p>Reimplemented from <a class="el" href="classcattle_1_1_activation_layer.html#abea6c88bbdb5cb6c77a69e2219e291b3">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a26107800b142f95b0d34a9519b801e9c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a26107800b142f95b0d34a9519b801e9c">&#9670;&nbsp;</a></span>init()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::init </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It initializes the layer and its parameters. </p>

<p>Reimplemented from <a class="el" href="classcattle_1_1_activation_layer.html#a03f8dc5fe0d781b983e4494ab8413f5f">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a50562bf1416371801c5abba6a1b206b0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a50562bf1416371801c5abba6a1b206b0">&#9670;&nbsp;</a></span>pass_back()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Root::Data <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::pass_back </td>
          <td>(</td>
          <td class="paramtype">typename Root::Data&#160;</td>
          <td class="paramname"><em>out_grads</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It back-propagates the derivative of the error function w.r.t. the output of the layer updating the gradient of its learnable parameters along the way if there are any. If there are, it also calculates the derivative of the regularization penalty w.r.t. to the layer's parameters and adds it to their gradient.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">out_grads</td><td>The derivative of the loss function w.r.t. the output of the layer </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The derivative of the loss function w.r.t. the output of the previous layer or a null tensor if the layer is an input layer. </dd></dl>

<p>Implements <a class="el" href="classcattle_1_1_layer.html#a10194a9e1de70a94e0e30a4701d14c0f">cattle::Layer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a2c4c775a8f711ecce2636d71b4d31c77"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c4c775a8f711ecce2636d71b4d31c77">&#9670;&nbsp;</a></span>pass_forward()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Root::Data <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::pass_forward </td>
          <td>(</td>
          <td class="paramtype">typename Root::Data&#160;</td>
          <td class="paramname"><em>in</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>training</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It has the function represented by the layer applied to the input tensor.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">in</td><td>A tensor representing a batch of observations. The observations are of the rank specified by the layer's template parameter and the input tensors rank is one greater. </td></tr>
    <tr><td class="paramname">training</td><td>Whether the input is to be processed in training or inference mode. If the forward pass is performed in inference mode, the backward pass is not guaranteed to work. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The output of the function represented by the layer applied to the input tensor. </dd></dl>

<p>Implements <a class="el" href="classcattle_1_1_layer.html#a6c1ea7b25d9f882364b7f2288f02d8da">cattle::Layer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<a id="a7bb2f7bd02261af5aae033d9727c00c8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7bb2f7bd02261af5aae033d9727c00c8">&#9670;&nbsp;</a></span>regularize()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Scalar, std::size_t Rank&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcattle_1_1_p_re_l_u_activation_layer.html">cattle::PReLUActivationLayer</a>&lt; Scalar, Rank &gt;::regularize </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p>It computes the derivative of the regularization function w.r.t. the parameters of the layer and adds it to their gradient. If the layer is not parametric, calling this method has no effect. </p>

<p>Reimplemented from <a class="el" href="classcattle_1_1_activation_layer.html#aff288c69e4af006db23d2e998d84e847">cattle::ActivationLayer&lt; Scalar, Rank &gt;</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>include/cattle/<a class="el" href="_layer_8hpp_source.html">Layer.hpp</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
