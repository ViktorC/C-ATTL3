A neural network library written in C++. C-\/\+A\+T\+T\+L3 uses \href{http://eigen.tuxfamily.org}{\tt Eigen}, the popular linear algebra library when run on the C\+PU. It allows for the easy construction and training of feed-\/forward neural networks ranging from simple M\+L\+Ps to state-\/of-\/the-\/art convolutional Inception\+Nets, Res\+Nets, Inception-\/\+Res\+Nets, and Dense\+Nets. C-\/\+A\+T\+T\+L3 also supports different floating point scalar types such as {\ttfamily float}, {\ttfamily double}, and {\ttfamily long double}.

The highest level building blocks of the different architectures are the neural network implementations provided by the library. Using these implementations, either as modules in a composite constellation or as standalone networks, almost any neural network architecture can be constructed. They are the following\+:
\begin{DoxyItemize}
\item Sequential\+Neural\+Network
\item Parallel\+Neural\+Network
\item Composite\+Neural\+Network
\item Residual\+Neural\+Network
\item Dense\+Neural\+Network
\end{DoxyItemize}

Sequential neural networks are ordinary networks with a set of layers through which the input is propagated. Parallel neural nets, on the other hand, contain one or more \textquotesingle{}lanes\textquotesingle{} of networks through which the input is simultaneously propagated and eventually concatenated along the depth dimension. Composite neural networks, similarly to parallel networks, are composed of one or more sub-\/nets; however, these nets are stacked sequentially. Residual networks and dense networks are implementations of the \href{https://arxiv.org/abs/1512.03385}{\tt Res\+Net} and \href{https://arxiv.org/abs/1608.06993}{\tt Dense\+Net} architectures that use composite neural networks as their sub-\/modules. Both the inputs and outputs of these neural networks are rank-\/four tensors which allow for the representation of batches of images or data of lower rank.

The lower level building blocks of neural networks are the layers. C-\/\+A\+T\+T\+L3 contains a wide selection of layers that can be used for the construction of highly effective sequential neural network modules. The available layer types are the following\+:
\begin{DoxyItemize}
\item F\+C\+Layer
\item Activation\+Layer
\begin{DoxyItemize}
\item Identity\+Activation\+Layer
\item Scaling\+Activation\+Layer
\item Binary\+Step\+Activation\+Layer
\item Sigmoid\+Activation\+Layer
\item Tanh\+Activation\+Layer
\item Softmax\+Activation\+Layer
\item Re\+L\+U\+Activation\+Layer
\item Leaky\+Re\+L\+U\+Activation\+Layer
\item E\+L\+U\+Activation\+Layer
\item P\+Re\+L\+U\+Activation\+Layer
\end{DoxyItemize}
\item Conv\+Layer
\item Pooling\+Layer
\begin{DoxyItemize}
\item Sum\+Pooling\+Layer
\item Max\+Pooling\+Layer
\item Mean\+Pooling\+Layer
\end{DoxyItemize}
\item Dropout\+Layer
\item Batch\+Norm\+Layer
\end{DoxyItemize}

Besides the input dimensions, the one parameter required by all, each layer uses multiple hyper-\/parameters (e.\+g. max-\/norm constraint, dilation, receptor field size, etc.). These parameters can be fine-\/tuned to optimize the behaviour of the networks. The fully-\/connected and convolutional layers also require weight initialization. The out-\/of-\/the-\/box weight initialization algorithms include\+:
\begin{DoxyItemize}
\item Zero\+Weight\+Initialization
\item One\+Weight\+Initialization
\item Le\+Cun\+Weight\+Initialization
\item Glorot\+Weight\+Initialization
\item He\+Weight\+Initialization
\item Orthogonal\+Weight\+Initialization
\end{DoxyItemize}

The library also provides optimizers that can be used to train the networks via backpropagation. The currently available (first-\/order gradient descent) optimizers include the following\+:
\begin{DoxyItemize}
\item Vanilla\+S\+G\+D\+Optimizer
\item Momentum\+Accelerated\+S\+G\+D\+Optimizer
\item Nesterov\+Momentum\+Accelerated\+S\+G\+D\+Optimizer
\item Adagrad\+Optimizer
\item R\+M\+S\+Prop\+Optimizer
\item Adadelta\+Optimizer
\item Adam\+Optimizer
\item Ada\+Max\+Optimizer
\item Nadam\+Optimizer
\end{DoxyItemize}

Similarly to the layers, these optimizers rely on hyper-\/parameters as well. Besides the hyper-\/parameters, optimizers also require more-\/or-\/less differentiable loss functions and regularization penalty functions. The library provides the following out of the box loss functions\+:
\begin{DoxyItemize}
\item Quadratic\+Loss
\item Hinge\+Loss
\item Cross\+Entropy\+Loss
\item Multi\+Label\+Hinge\+Loss
\item Multi\+Label\+Log\+Loss
\end{DoxyItemize}

The standard regularization penalties are\+:
\begin{DoxyItemize}
\item No\+Regularization\+Penalty
\item L1\+Regularization\+Penalty
\item L2\+Regularization\+Penalty
\item Elastic\+Net\+Regularization\+Penalty
\end{DoxyItemize}

Given these parameters, optimizers can be constructed and used for gradient checks (in case of using self-\/implemented sub-\/classes of the core interfaces) and network training. Both methods are parameterized by a neural network implementation and one or two data providers. Data providers are responsible for supplying the data used for gradient verification, training, and testing. Currently only an in-\/memory data provider implementation is provided by the library, but the addition of a general on-\/disk data provider and specialized providers for popular data sets such as M\+N\+I\+ST, C\+I\+F\+AR, and Image\+Net is planned.

C-\/\+A\+T\+T\+L3 also contains two preporcessors that can be used to transform the input data. They are\+:
\begin{DoxyItemize}
\item Normalization\+Preprocessor
\item P\+C\+A\+Preprocessor
\end{DoxyItemize}

Once a neural network has been trained, it can be used for inference effortlessly. The following code snippet demonstrates the usage of the library via a simple example. \begin{DoxyVerb}using namespace cattle;

// Generate random training data
Tensor4Ptr<Scalar> training_obs_ptr = Tensor4Ptr<Scalar>(new Tensor4<Scalar>(80, 32, 32, 3));
Tensor4Ptr<Scalar> training_obj_ptr = Tensor4Ptr<Scalar>(new Tensor4<Scalar>(80, 1, 1, 1));
training_obs_ptr->setRandom();
training_obj_ptr->setRandom();
InMemoryDataProvider<Scalar> training_prov(std::move(training_obs_ptr), std::move(training_obj_ptr));

// Generate random test data
Tensor4Ptr<Scalar> test_obs_ptr = Tensor4Ptr<Scalar>(new Tensor4<Scalar>(20, 32, 32, 3));
Tensor4Ptr<Scalar> test_obj_ptr = Tensor4Ptr<Scalar>(new Tensor4<Scalar>(20, 1, 1, 1));
test_obs_ptr->setRandom();
test_obj_ptr->setRandom();
InMemoryDataProvider<Scalar> test_prov(std::move(test_obs_ptr), std::move(test_obj_ptr));

// Construct a simple convolutional neural network.
WeightInitSharedPtr<Scalar> init(new HeWeightInitialization<Scalar>());
std::vector<LayerPtr<Scalar>> layers(9);
layers[0] = LayerPtr<Scalar>(new ConvLayer<Scalar>(training_prov.get_obs_dims(), 10, init, 5, 2));
layers[1] = LayerPtr<Scalar>(new ReLUActivationLayer<Scalar>(layers[0]->get_output_dims()));
layers[2] = LayerPtr<Scalar>(new MaxPoolingLayer<Scalar>(layers[1]->get_output_dims()));
layers[3] = LayerPtr<Scalar>(new ConvLayer<Scalar>(layers[2]->get_output_dims(), 20, init));
layers[4] = LayerPtr<Scalar>(new ReLUActivationLayer<Scalar>(layers[3]->get_output_dims()));
layers[5] = LayerPtr<Scalar>(new MaxPoolingLayer<Scalar>(layers[4]->get_output_dims()));
layers[6] = LayerPtr<Scalar>(new FCLayer<Scalar>(layers[5]->get_output_dims(), 500, init));
layers[7] = LayerPtr<Scalar>(new ReLUActivationLayer<Scalar>(layers[6]->get_output_dims()));
layers[8] = LayerPtr<Scalar>(new FCLayer<Scalar>(layers[7]->get_output_dims(), 1, init));
SequentialNeuralNetwork<Scalar> nn(std::move(layers));

// Initialize the network.
nn.init();

// Construct the optimizer.
LossSharedPtr<Scalar> loss(new QuadraticLoss<Scalar>());
RegPenSharedPtr<Scalar> reg(new ElasticNetRegularizationPenalty<Scalar>());
NadamOptimizer<Scalar> opt(loss, reg, 20);

// Train the network for 500 epochs.
opt.optimize(nn, training_prov, test_prov, 500);

// Generate random input data.
Tensor4<Scalar> input(5, 32, 32, 3);
input.setRandom();

// Inference
Tensor4<Scalar> prediction = nn.infer(input);
\end{DoxyVerb}


Planned features include additional data providers, network serialization and de-\/serialization, L\+S\+TM and G\+RU layers, evolutionary and second order optimization algorithms, and G\+PU support via the use of cu\+B\+L\+AS and cu\+D\+NN. 